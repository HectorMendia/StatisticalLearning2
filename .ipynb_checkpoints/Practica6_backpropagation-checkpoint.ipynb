{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 6\n",
    "### Back-propagation (Propagacion del error)\n",
    "\n",
    "Dado el siguiente diagrama y asumiendo que las neuronas de salida tienen errores de : [3.0,10.0], calcule el término de error(delta minúscula) simplificado para cada una de las neuronas en el diagrama.\n",
    "\n",
    "<img src='back1.png' width='50%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso lo primero que se calculo fue el forward propagation\n",
    "\n",
    "Tomando como:\n",
    "\n",
    "* x1 = x2 = x3 = 1    \n",
    "* Learning rate = 1\n",
    "* Activacion = sigmoid = $ f(x) =  \\frac{1}{1 + e^-x }  $\n",
    "\n",
    "|     |      |    |      | | |\n",
    "|-----|------|-----|-------|-----|-------|\n",
    "| w1  | 0.3  | w10 | -0.3  | w19 | -0.23 |\n",
    "| w2  | 0.46 | w11 | 0.62  | w20 | 0.22 |\n",
    "| w3  | 0.02 | w12 | 0.45  | w21 | 0.77 |\n",
    "| w4  | 0.22 | w13 | 0.45  | w22 | 0.9 |\n",
    "| w5  | -0.7 | w14 | 0.57  | w23 | 0.88 |\n",
    "| w6  | 0.65 | w15 | 0.48  | w24 | -0.4 |\n",
    "| w7  | 0.65 | w16 | 0.65  |  |  |\n",
    "| w8  | 0.9  | w17 | 0.58  |  |  |\n",
    "| w9  | 0.34 | w18 | -0.45 |  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H1 = x1*w1 + x2*w2 + x3*w3 = 0.3 + 0.46 + 0.02 = 0.78 --> sigmoid(H1) = 0.6856$\n",
    "\n",
    "Por lo que tendriamos\n",
    "\n",
    "|     |      |    |      | | |\n",
    "|-----|------|-----|-------|-----|-------|\n",
    "| H1  | 0.685680114 | H4 | 0.62750375   | Y1 | 0.616164076 |\n",
    "| H2  | 0.542397941 | H5 | 0.737831013  | Y2 | 0.726616765 |\n",
    "| H3  | 0.868755531 | H6 | 0.591300188  |  |  |\n",
    "\n",
    "Los errores datos fueron 3, 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation de cada w**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha = 1$\n",
    "$$ w19 = w19 - \\alpha\\frac{\\partial error}{\\partial w19} $$\n",
    "\n",
    "$$ \\frac{\\partial error}{\\partial w19} = \\frac{\\partial error}{\\partial aY_1} \\frac{\\partial aY_1}{\\partial Y_1} \\frac{\\partial Y_1}{\\partial w19} $$\n",
    "\n",
    "$$\\frac{\\partial error}{\\partial aY_1} = 3$$\n",
    "$$\\frac{\\partial aY_1}{\\partial Y_1} = aY_1(1-aY_1) = 0.616164076 (1-0.616164076) = 0.236505907$$\n",
    "$$\\frac{\\partial Y_1}{\\partial w19} = aH4 = 0.62750375$$\n",
    "$$ \\frac{\\partial error}{\\partial w19} = (3) (0.236505907) (0.62750375) = 0.445225032$$\n",
    "\n",
    "$$ w19 = w19 - \\alpha \\frac{\\partial error}{\\partial w19} = -0.23 - 0.445225032 = -0.675225032$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     | inicial | cambio | nuevo (w-cambio)|\n",
    "|-----|---------|--------|-------|\n",
    "| w19 |  -0.23 | 0.445225032 | -0.675225032 |\n",
    "| w20 |  0.22  | 0.52350418  | -0.30350418  |\n",
    "| w21 |  0.77  | 0.419537963 | 0.350462037  |\n",
    "| w22 |  0.9   | 1.246503832 | -0.346503832 |\n",
    "| w23 |  0.88  | 1.465663248 | -0.585663248 |\n",
    "| w24 |  -0.4  | 1.174587323 | -1.574587323 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ w10 = w10 - \\alpha\\frac{\\partial error}{\\partial w10} $$\n",
    "<br/>\n",
    "$$ \\frac{\\partial error}{\\partial w10} = \\frac{\\partial error}{\\partial aH4} \\frac{\\partial aH4}{\\partial H4} \\frac{\\partial H4}{\\partial w10} = (1.6246145) (0.233742794) (0.685680114) = 0.260381491$$\n",
    "<br/>\n",
    "\n",
    "$$\\frac{\\partial H4}{\\partial w10} = aH1 = 0.685680114$$\n",
    "$$\\frac{\\partial aH4}{\\partial H4} = aH4(1-aH4) = 0.62750375 (1-0.62750375) = 0.233742794$$\n",
    "$$\\frac{\\partial error}{\\partial aH4} = \\frac{\\partial error1}{\\partial aH4} + \\frac{\\partial error2}{\\partial aH4} = (-0.163189076) + (1.787803576) = 1.6246145 $$ \n",
    "<br/>\n",
    "\n",
    "$$\\frac{\\partial error1}{\\partial aH4} = \\frac{\\partial error1}{\\partial Y_1} \\frac{\\partial Y_1}{\\partial aH4} = (0.709517722)(-0.23) = -0.163189076 $$ \n",
    "$$\\frac{\\partial error1}{\\partial Y_1} = \\frac{\\partial error1}{\\partial aY_1} \\frac{\\partial aY_1}{\\partial Y_1} = (3)(0.236505907) = 0.709517722$$\n",
    "$$\\frac{\\partial Y_1}{\\partial aH4} = w19 = -0.23$$\n",
    "<br/>\n",
    "$$\\frac{\\partial error2}{\\partial aH4} = \\frac{\\partial error2}{\\partial Y_2} \\frac{\\partial Y_2}{\\partial aH4} = (0.198644842)(0.9) = 1.787803576 $$ \n",
    "$$\\frac{\\partial error2}{\\partial Y_2} = \\frac{\\partial error2}{\\partial aY_2} \\frac{\\partial aY_2}{\\partial Y_1} = (10)(0.198644842) = 0.198644842$$\n",
    "$$\\frac{\\partial Y_2}{\\partial aH4} = w22 = 0.9$$\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "$$ w10 = w10 - \\alpha \\frac{\\partial error}{\\partial w10} = -0.3 - 0.260381491 = -0.560381491$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar a w10 se calcula w11 y w12\n",
    "\n",
    "<br/>\n",
    "$$ \\frac{\\partial error}{\\partial w11} = \\frac{\\partial error}{\\partial aH4} \\frac{\\partial aH4}{\\partial H4} \\frac{\\partial H4}{\\partial w11} = 0.205971242$$\n",
    "\n",
    "<br/>\n",
    "$$ \\frac{\\partial error}{\\partial w12} = \\frac{\\partial error}{\\partial aH4} \\frac{\\partial aH4}{\\partial H4} \\frac{\\partial H4}{\\partial w12} = 0.329902903$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para w13\n",
    "\n",
    "<br/>\n",
    "$$ \\frac{\\partial error}{\\partial w13} = \\frac{\\partial error}{\\partial aH5} \\frac{\\partial aH5}{\\partial H5} \\frac{\\partial H5}{\\partial w13} = 0.252560341$$\n",
    "\n",
    "<br/>\n",
    "$$\\frac{\\partial H5}{\\partial w13} = aH1 = 0.685680114$$\n",
    "$$\\frac{\\partial aH5}{\\partial H5} = aH5 (1 - aH5) = 0.737831013 (1 - 0.737831013) = 0.193436409$$\n",
    "$$\\frac{\\partial error}{\\partial aH5} = \\frac{\\partial error1}{\\partial aH5} + \\frac{\\partial error2}{\\partial aH5} = (0.156093899) + (1.748074608) = 1.904168507 $$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valores de w\n",
    "\n",
    "|     | inicial | cambio | nuevo (w-cambio) |\n",
    "|-----|---------|--------|-------|\n",
    "| w10 |  -0.3  | 0.260381491 | -0.560381491 |\n",
    "| w11 |  0.62  | 0.205971242 | 0.414028758  |\n",
    "| w12 |  0.45  | 0.329902903 | 0.120097097  |\n",
    "| w13 |  0.45  | 0.252560341 | 0.197439659 |\n",
    "| w14 |  0.57  | 0.199784427 | 0.370215573 |\n",
    "| w15 |  0.48  | 0.319993519 | 0.160006481 |\n",
    "| w16 |  0.65  | -0.041136234 | 0.691136234 |\n",
    "| w17 |  0.58  | -0.032540259 | 0.612540259 |\n",
    "| w18 |  -0.45 | -0.052119538 | -0.397880462 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la ultima parte w1\n",
    "\n",
    "$$ w1 = w1 - \\alpha\\frac{\\partial error}{\\partial w1} $$\n",
    "<br/>\n",
    "$$ \\frac{\\partial error}{\\partial w1} = \\frac{\\partial error}{\\partial aH1} \\frac{\\partial aH1}{\\partial H1} \\frac{\\partial H1}{\\partial w1} = (0.012832739) (0.215522895) (1) = 0.002765749$$\n",
    "<br/>\n",
    "\n",
    "$$\\frac{\\partial H1}{\\partial w1} = x1 = 1$$\n",
    "$$\\frac{\\partial aH1}{\\partial H1} = aH1(1-aH1) = 0.685680114 (1 - 0.685680114) = 0.215522895$$\n",
    "$$\\frac{\\partial error}{\\partial aH1} = \\frac{\\partial error1}{\\partial aH1} + \\frac{\\partial error2}{\\partial aH1} + \\frac{\\partial error3}{\\partial aH1} = (-0.11392258) + (0.165750984) + (-0.038995665) = 0.012832739 $$ \n",
    "<br/>\n",
    "\n",
    "$$\\frac{\\partial error1}{\\partial aH1} = \\frac{\\partial error1}{\\partial H4} \\frac{\\partial H4}{\\partial aH1} = (0.379741932)(-0.3) = -0.11392258$$ \n",
    "$$\\frac{\\partial error1}{\\partial H4} = \\frac{\\partial error1}{\\partial aH4} \\frac{\\partial aH4}{\\partial H4} = (1.6246145)(0.233742794) = 0.379741932$$\n",
    "$$\\frac{\\partial H4}{\\partial aH1} = w10 = -0.3$$\n",
    "<br/>\n",
    "$$\\frac{\\partial error2}{\\partial aH1} = \\frac{\\partial error2}{\\partial H5} \\frac{\\partial H5}{\\partial aH1} = (0.368335519)(0.45) = 0.165750984$$ \n",
    "$$\\frac{\\partial error2}{\\partial H5} = \\frac{\\partial error2}{\\partial aH5} \\frac{\\partial aH5}{\\partial H5} = (1.904168507)(0.193436409) = 0.368335519$$\n",
    "$$\\frac{\\partial H5}{\\partial aH1} = w13 = 0.45$$\n",
    "<br/>\n",
    "$$\\frac{\\partial error3}{\\partial aH1} = \\frac{\\partial error3}{\\partial H6} \\frac{\\partial H6}{\\partial aH1} = (-0.059993331)(0.65) = -0.038995665$$ \n",
    "$$\\frac{\\partial error3}{\\partial H6} = \\frac{\\partial error3}{\\partial aH6} \\frac{\\partial aH6}{\\partial H6} = (-0.248250721)(0.241664276) = -0.059993331$$\n",
    "$$\\frac{\\partial H6}{\\partial aH1} = w16 = 0.65$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "$$ w10 = w10 - \\alpha \\frac{\\partial error}{\\partial w10} = -0.3 - 0.260381491 = -0.560381491$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambios en w\n",
    "\n",
    "|     | inicial | cambio | nuevo |\n",
    "|-----|---------|--------|-------|\n",
    "| w1 | 0.3  | 0.002765749 | 0.297234251 |\n",
    "| w2 | 0.46 | 0.002765749 | 0.457234251 |\n",
    "| w3 | 0.02 | 0.002765749 | 0.017234251 |\n",
    "| w4 | 0.22 | 0.101910698 | 0.118089302 |\n",
    "| w5 | -0.7 | 0.101910698 | -0.801910698 |\n",
    "| w6 | 0.65 | 0.101910698 | 0.548089302 |\n",
    "| w7 | 0.65 | 0.042720992 | 0.607279008 |\n",
    "| w8 | 0.9  | 0.042720992 | 0.857279008 |\n",
    "| w9 | 0.34 | 0.042720992 | 0.297279008 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-propagation(Práctica)\n",
    "\n",
    "* Entrenar un aproximador para la función Xor usando 2 capas intermedias.\n",
    "* Usar 2 neuronas en la capa anterior a la salida(segunda capa oculta)\n",
    "* Usar al menos 2(pueden ser más) en la primera capa oculta.\n",
    "* Usar activación ReLu en las capas intermedias y no activación en la salida\n",
    "* Usar numpy\n",
    "* Realizar 5 experimentos, en cada experimento(corrida de entrenamiento):\n",
    "* Inicializar los parámetros aleatoriamente con distribución normal centrada en 0 y std = 0.1\n",
    "* Retornar la representación intermedia de la segunda capa oculta.\n",
    "* Graficar las 5 representaciones intermedias(1 por experimento), comparar, comentar y/o concluir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ident(Z):\n",
    "    A = Z\n",
    "    return A\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(n_x, n_h, n_y):\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.normal(0, 0.1, size=(n_h,n_x))\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.normal(0, 0.1, size=(n_h,n_x)) \n",
    "    b2 = np.zeros((n_y,1))\n",
    "    W3 = np.random.normal(0, 0.1, size=(n_y,n_h)) \n",
    "    b3 = np.zeros((n_y,1))\n",
    "\n",
    "    \n",
    "    salida = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return salida    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(vals, W, b, tipo):\n",
    "    Z = np.dot(W,vals)+b\n",
    "    if tipo == \"ident\":\n",
    "        salida = ident(Z)\n",
    "    elif tipo == \"relu\":\n",
    "        salida = relu(Z)\n",
    "        \n",
    "    return salida,Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = - np.sum(AL-Y)\n",
    "    cost = cost/m\n",
    "    cost = np.squeeze(cost) # reducir dimensionalidad del array\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, A_prev, w, b):\n",
    "    m = A_prev.shape[1]\n",
    "    dw = np.dot(dZ,A_prev.T)/m\n",
    "\n",
    "    db = np.array([[np.sum(dZ)/m]])  #np.sum(dZ,axis=1,keepdims=True)/m #np.array([[np.sum(dZ)/m]])\n",
    "    dA_prev = np.dot(w.T,dZ)\n",
    "    \n",
    "    return dA_prev, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, A, W, B, Z,tipo):\n",
    "    \n",
    "    if tipo == \"ident\":\n",
    "        dZ = np.ones(dA.shape)\n",
    "    elif tipo == \"relu\":\n",
    "        #dZ = np.where(dA <= 0, 0, 1)\n",
    "        dZ = np.greater(dA, 0).astype(int)\n",
    "        \n",
    "    dA_prev, dW, db = linear_backward(dZ, A, W, B)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 \n",
    "    for l in range(1,L+1):\n",
    "        parameters[\"W\" + str(l)]= parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)]= parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0]\n",
      " [1 0 1 0]] [[ 0.16243454 -0.06117564]\n",
      " [-0.05281718 -0.10729686]] [[0.]\n",
      " [0.]]\n",
      "**\n",
      "[[ 0.08654076 -0.23015387]\n",
      " [ 0.17448118 -0.07612069]] [[0.]]\n",
      "**\n",
      "[[ 0.03190391 -0.02493704]] [[0.]]\n",
      "cost:  0.5001048217627753\n",
      "Y:  [[0 1 1 0]]\n",
      "A2:  [[-0.00016101 -0.00025828  0.          0.        ]]\n",
      "dA2:  [[-1.61007209e-04 -1.00025828e+00 -1.00000000e+00 -0.00000000e+00]]\n",
      "Cost after iteration 0: 0.5001048217627753\n",
      "cost:  0.6000000000000001\n",
      "Y:  [[0 1 1 0]]\n",
      "A2:  [[-0.1 -0.1 -0.1 -0.1]]\n",
      "dA2:  [[-0.1 -1.1 -1.1 -0.1]]\n",
      "cost:  0.7\n",
      "Y:  [[0 1 1 0]]\n",
      "A2:  [[-0.2 -0.2 -0.2 -0.2]]\n",
      "dA2:  [[-0.2 -1.2 -1.2 -0.2]]\n",
      "cost:  0.8\n",
      "Y:  [[0 1 1 0]]\n",
      "A2:  [[-0.3 -0.3 -0.3 -0.3]]\n",
      "dA2:  [[-0.3 -1.3 -1.3 -0.3]]\n",
      "cost:  0.8999999999999999\n",
      "Y:  [[0 1 1 0]]\n",
      "A2:  [[-0.4 -0.4 -0.4 -0.4]]\n",
      "dA2:  [[-0.4 -1.4 -1.4 -0.4]]\n",
      "cost:  1.0\n",
      "Y:  [[0 1 1 0]]\n",
      "A2:  [[-0.5 -0.5 -0.5 -0.5]]\n",
      "dA2:  [[-0.5 -1.5 -1.5 -0.5]]\n",
      "Cost after iteration 5: 1.0\n",
      "cost:  1.1\n",
      "Y:  [[0 1 1 0]]\n",
      "A2:  [[-0.6 -0.6 -0.6 -0.6]]\n",
      "dA2:  [[-0.6 -1.6 -1.6 -0.6]]\n",
      "cost:  1.2\n",
      "Y:  [[0 1 1 0]]\n",
      "A2:  [[-0.7 -0.7 -0.7 -0.7]]\n",
      "dA2:  [[-0.7 -1.7 -1.7 -0.7]]\n",
      "cost:  1.2999999999999998\n",
      "Y:  [[0 1 1 0]]\n",
      "A2:  [[-0.8 -0.8 -0.8 -0.8]]\n",
      "dA2:  [[-0.8 -1.8 -1.8 -0.8]]\n",
      "cost:  1.4\n",
      "Y:  [[0 1 1 0]]\n",
      "A2:  [[-0.9 -0.9 -0.9 -0.9]]\n",
      "dA2:  [[-0.9 -1.9 -1.9 -0.9]]\n",
      "cost:  1.5\n",
      "Y:  [[0 1 1 0]]\n",
      "A2:  [[-1. -1. -1. -1.]]\n",
      "dA2:  [[-1. -2. -2. -1.]]\n",
      "Cost after iteration 10: 1.5\n",
      "salida:  [[-1. -1. -1. -1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_x = 2\n",
    "n_h = 2\n",
    "n_y = 1\n",
    "num_iterations = 11\n",
    "learning_rate = 0.1\n",
    "print_cost=True\n",
    "#layers_dims = (n_x, n_h, n_y)\n",
    "\n",
    "X = np.array([[1,1],[1,0],[0,1],[0,0]]).T\n",
    "Y = np.array([[0,1,1,0]])\n",
    "\n",
    "np.random.seed(1)\n",
    "grads = {}\n",
    "costs = []                              # to keep track of the cost\n",
    "m = X.shape[1]                           # number of examples\n",
    "#(n_x, n_h, n_y) = layers_dims\n",
    "\n",
    "# Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "### START CODE HERE ### (≈ 1 line of code)\n",
    "parameters = init(n_x, n_h, n_y) \n",
    "### END CODE HERE ###\n",
    "\n",
    "# Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "W1 = parameters[\"W1\"]\n",
    "b1 = parameters[\"b1\"]\n",
    "W2 = parameters[\"W2\"]\n",
    "b2 = parameters[\"b2\"]\n",
    "W3 = parameters[\"W3\"]\n",
    "b3 = parameters[\"b3\"]\n",
    "print(X, W1, b1)\n",
    "print('**')\n",
    "print(W2, b2)\n",
    "print('**')\n",
    "print(W3, b3)\n",
    "\n",
    "# Loop (gradient descent)\n",
    "\n",
    "for i in range(0, num_iterations):\n",
    "\n",
    "    # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1\". Output: \"A1, cache1, A2, cache2\".\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    #print(X, W1, b1)\n",
    "    #A1, X,W1,b1 = forward_propagation(X, W1, b1, \"relu\")\n",
    "    #A2, a2,w2,b2 = forward_propagation(A1, W2, b2,\"ident\")\n",
    "    #A3, a2,w2,b2 = forward_propagation(A1, W2, b2,\"ident\")\n",
    "    A1,Z1 = forward_propagation(X, W1, b1, \"relu\")\n",
    "    A2,Z2 = forward_propagation(A1, W2, b2,\"relu\")\n",
    "    A3,Z3 = forward_propagation(A2, W3, b3,\"ident\")\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Compute cost\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    cost = compute_cost(A3, Y)\n",
    "    print('cost: ',cost)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Initializing backward propagation\n",
    "    \n",
    "    #dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "    dA3 = -(Y - A3)\n",
    "    print('Y: ', Y)\n",
    "    print('A2: ', A3)\n",
    "    print('dA2: ', dA3)\n",
    "    # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dA2, dW3, db3 = linear_activation_backward(dA3, A3, W3, b3, Z3, \"ident\")\n",
    "    #print('dA1: ', dA1.shape)\n",
    "    dA1, dW2, db2 = linear_activation_backward(dA2, A2, W2, b2, Z2, \"relu\")\n",
    "    \n",
    "    dA0, dW1, db1 = linear_activation_backward(dA1, A1, W1, b1, Z1, \"relu\")\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "    grads['dW1'] = dW1\n",
    "    grads['db1'] = db1\n",
    "    grads['dW2'] = dW2\n",
    "    grads['db2'] = db2\n",
    "    grads['dW3'] = dW3\n",
    "    grads['db3'] = db3\n",
    "\n",
    "    # Update parameters.\n",
    "    ### START CODE HERE ### (approx. 1 line of code)\n",
    "    parameters = update_parameters(parameters, grads, learning_rate)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Retrieve W1, b1, W2, b2 from parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "\n",
    "    # Print the cost every 100 training example\n",
    "    if print_cost and i % 5 == 0:\n",
    "        print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "    if print_cost and i % 5 == 0:\n",
    "        costs.append(cost)\n",
    "\n",
    "# plot the cost\n",
    "print('salida: ', A3)\n",
    "plt.plot(np.squeeze(costs))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-38.80339624247857"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([[0.00876302, 0.01405721, 0.,         0.        ]])\n",
    "x[x==0] = 1e-15\n",
    "np.nansum(np.multiply(np.log(x),np.array([[0, 1, 1, 0]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "(1, 2) (2, 4)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1,1],[1, 0], [0, 1], [0, 0]] )\n",
    "w = np.array([[ 0.16243454, -0.06117564], [-0.05281718, -0.10729686]])\n",
    "\n",
    "x = np.array([[1,1],[1, 0], [0, 1], [0, 0]] ).T\n",
    "w = np.array([[ 0.16243454, -0.06117564]])\n",
    "\n",
    "\n",
    "b= np.array([[0.], [0.]])\n",
    "print(x.shape[1])\n",
    "print(w.shape,x.shape)\n",
    "np.dot(w,x)\n",
    "y = np.array([[0,1,1,0]])\n",
    "print(y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(2,np.array([1,2]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.array([[1,1],[1,0],[0,1],[0,0]])\n",
    "\n",
    "\n",
    "type(1) == int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 1\n",
      "[[ 0.16243454 -0.06117564]\n",
      " [-0.05281718 -0.10729686]]\n",
      "[[ 0.08654076 -0.23015387]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "    \n",
    "W1 = np.random.randn(n_h,n_x) * 0.1\n",
    "W2 = np.random.randn(n_y,n_h) * 0.1\n",
    "\n",
    "print(n_h,n_x,n_y)\n",
    "print(W1)\n",
    "print(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08654076, -0.23015387]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "np.random.normal(0, 0.1, size=(2, 2))\n",
    "np.random.normal(0, 0.1, size=(1, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
